{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from matplotlib import pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "随机生成了一个10臂伯努利老虎机\n",
      "获奖概率最大的拉杆为1号,其获奖概率为0.7203\n"
     ]
    }
   ],
   "source": [
    "class BernoulliBandit:\n",
    "    \"\"\" 伯努利多臂老虎机,输入K表示拉杆个数 \"\"\"\n",
    "    def __init__(self, K):\n",
    "        self.probs = np.random.uniform(size=K)  # 随机生成K个0～1的数,作为拉动每根拉杆的获奖\n",
    "        # 概率\n",
    "        self.best_idx = np.argmax(self.probs)  # 获奖概率最大的拉杆\n",
    "        self.best_prob = self.probs[self.best_idx]  # 最大的获奖概率\n",
    "        self.K = K\n",
    "\n",
    "    def step(self, k):\n",
    "        # 当玩家选择了k号拉杆后,根据拉动该老虎机的k号拉杆获得奖励的概率返回1（获奖）或0（未\n",
    "        # 获奖）\n",
    "        if np.random.rand() < self.probs[k]:\n",
    "            return 1\n",
    "        else:\n",
    "            return 0\n",
    "\n",
    "\n",
    "np.random.seed(1)  # 设定随机种子,使实验具有可重复性\n",
    "K = num = 10\n",
    "bandit_10_arm = BernoulliBandit(K)\n",
    "print(\"随机生成了一个%d臂伯努利老虎机\" % K)\n",
    "print(\"获奖概率最大的拉杆为%d号,其获奖概率为%.4f\" %\n",
    "      (bandit_10_arm.best_idx, bandit_10_arm.best_prob))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "模拟估计奖励的过程，使用增量式更新。单个拉杆 $a$ 的期望奖励使用前面奖励 $r_a^k$ 的平均数估计，即\n",
    "\n",
    "$$\n",
    "\\hat{Q} (a) = \\frac{1}{N(a)} \\sum_{k = 1}^n r_a^k\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Estimator:\n",
    "    def __init__(self, machine: BernoulliBandit):\n",
    "        self.num = machine.K\n",
    "        self.agent = machine\n",
    "        self.counts = np.zeros(self.num, dtype=np.int32)\n",
    "        self.rewards = np.zeros(self.num, dtype=np.float64)\n",
    "    \n",
    "    def run(self, round: int):\n",
    "        \"\"\"\n",
    "        Estimate for a given round\n",
    "        \"\"\"\n",
    "        samples = np.random.randint(0, self.num, size=round)\n",
    "        for a in samples:\n",
    "            r = self.agent.step(a)\n",
    "            self.counts[a] += 1\n",
    "            self.rewards[a] += (r - self.rewards[a]) / self.counts[a]\n",
    "    \n",
    "    def get_estimation(self):\n",
    "        return self.rewards\n",
    "\n",
    "\n",
    "estimator = Estimator(bandit_10_arm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Estimation: [0.432 0.713 0.    0.301 0.16  0.093 0.193 0.343 0.408 0.515]\n",
      "Ground truth: [0.417 0.72  0.    0.302 0.147 0.092 0.186 0.346 0.397 0.539]\n"
     ]
    }
   ],
   "source": [
    "estimator.run(10000)\n",
    "np.set_printoptions(precision=3, suppress=True)\n",
    "print(\"Estimation:\", estimator.get_estimation())\n",
    "print(\"Ground truth:\", bandit_10_arm.probs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "懊悔（regret）：定义为拉动一个拉杆 $k$ 时，最优拉杆与当前选择拉杆的期望之差；\n",
    "累计懊悔（cumulative regret）定义为每次行动的懊悔之和"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Solver:\n",
    "    def __init__(self, bandit: BernoulliBandit):\n",
    "        self.bandit = bandit\n",
    "        self.num = bandit.K\n",
    "        self.counts = np.zeros(self.num, dtype=np.int32)\n",
    "        self.regret = 0. # Cumulative regret\n",
    "        self.actions = []\n",
    "        self.regrets = []\n",
    "    \n",
    "    def run_one_step(self):\n",
    "        \"\"\"\n",
    "        选择一个动作\n",
    "        此时没有 state，不需要输入\n",
    "        \"\"\"\n",
    "        a = np.random.randint(0, self.num)\n",
    "        return a\n",
    "    \n",
    "    def update_regret(self, action: int):\n",
    "        regret = self.bandit.best_prob - self.bandit.probs[action]\n",
    "        self.regret += regret\n",
    "        self.regrets.append(regret)\n",
    "\n",
    "    def run(self, num_steps: int):\n",
    "        for _ in range(num_steps):\n",
    "            a = self.run_one_step()\n",
    "            self.counts[a] += 1\n",
    "            self.update_regret(a)\n",
    "            self.actions.append(a)\n",
    "\n",
    "solver = Solver(bandit_10_arm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "solver.run(10000)\n",
    "solver.regret"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "torch",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
