"""
Generated by deepseek
"""

import os
from typing import Optional, Callable

import torch
from torch.utils.data import Dataset, DataLoader
from torchvision.transforms import ToTensor
from diffusers import DDPMPipeline, DDPMScheduler, UNet2DConditionModel
from transformers import CLIPTextModel, CLIPTokenizer
import torch
import json
from PIL import Image
from tqdm import tqdm


learning_rate = 0.001
batch_size = 32
num_epochs = 1000
device = torch.device("cuda")

class TextToImageDataset(Dataset):
    def __init__(self, dir_path: str, meta_file: str, transforms: Optional[Callable]=None):
        super().__init__()
        with open(meta_file) as f:
            files = f.read().split('\n')
        self.dataset = []
        self.transforms = transforms
        
        for filename in tqdm(files, desc="Image Loading"):
            image = Image.open(os.path.join(dir_path, filename + ".jpg"))
            if transforms:
                image = transforms(image)
            with open(os.path.join(dir_path, filename + ".json")) as json_file:
                prompt_json = json.load(json_file)
            prompt = prompt_json["prompt"]
            self.dataset.append({"image": image, "prompt": prompt})
        
    def __getitem__(self, index):
        return self.dataset[index]
    
    def __len__(self):
        return len(self.dataset)


dataset = TextToImageDataset(
    "/home/sunlight/learning/diffusers/dataset/text2image_lr_dataset",
    "/home/sunlight/learning/diffusers/dataset/image_list.txt",
    ToTensor()
)
train_dataloader = DataLoader(dataset, batch_size=batch_size, shuffle=True)

unet = UNet2DConditionModel(
    sample_size=64,
    in_channels=3,
    out_channels=3,
    layers_per_block=2,
    block_out_channels=(32, 64, 64, 64),
    cross_attention_dim=512,
    down_block_types=(
        "DownBlock2D",
        "CrossAttnDownBlock2D",  # 支持条件输入
        "CrossAttnDownBlock2D",
        "DownBlock2D",
    ),
    up_block_types=(
        "UpBlock2D",
        "CrossAttnUpBlock2D",
        "CrossAttnUpBlock2D",
        "UpBlock2D",
    ),
)

noise_scheduler = DDPMScheduler(num_train_timesteps=1000)
optimizer = torch.optim.AdamW(unet.parameters(), lr=learning_rate)

clip_model_name = "openai/clip-vit-base-patch32"
tokenizer = CLIPTokenizer.from_pretrained(clip_model_name)
text_encoder = CLIPTextModel.from_pretrained(clip_model_name)

unet.to(device)

global_step = 0
for epoch in tqdm(range(num_epochs), desc="Train epoch"):
    unet.train()
    for step, batch in tqdm(enumerate(train_dataloader), desc="batch", total=len(train_dataloader)):
        clean_images = batch["image"]
        text_labels = batch["prompt"]
        
        # 添加噪声
        noise = torch.randn_like(clean_images)
        timesteps = torch.randint(0, noise_scheduler.config.num_train_timesteps, (clean_images.shape[0],)).long()
        noisy_images = noise_scheduler.add_noise(clean_images, noise, timesteps)
        
        # 条件生成：将文本标签作为模型输入
        text_inputs = tokenizer(
            text_labels, 
            padding="max_length",      # 填充到统一长度
            max_length=tokenizer.model_max_length,  # 如 77
            truncation=True,           # 超长文本截断
            return_tensors="pt"        # 返回 PyTorch 张量
        )
        input_ids = text_inputs.input_ids
        with torch.no_grad():
            encoder_hidden_states = text_encoder(input_ids).last_hidden_state
        
        # 预测噪声残差
        noisy_images = noisy_images.to(device)
        timesteps = timesteps.to(device)
        encoder_hidden_states = encoder_hidden_states.to(device)
        noise = noise.to(device)

        noise_pred = unet(noisy_images, timesteps, encoder_hidden_states).sample
        
        # 计算损失
        optimizer.zero_grad()
        loss = torch.nn.functional.mse_loss(noise_pred, noise)
        loss.backward()
        optimizer.step()
        global_step += 1

    if epoch % 100 == 99:
        pipeline = DDPMPipeline(
            unet=unet,
            scheduler=noise_scheduler,
        )
        pipeline.save_pretrained(f"ckpt/ddpm-checkpoint-{epoch}")
