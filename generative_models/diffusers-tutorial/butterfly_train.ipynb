{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from diffusers import StableDiffusionPipeline\n",
    "import torchvision\n",
    "from datasets import load_dataset\n",
    "from torchvision import transforms\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import torch.nn.functional as F\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pytorch_lightning as pl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "dataset = load_dataset(\"huggan/smithsonian_butterflies_subset\", split=\"train\")\n",
    "\n",
    "# Or load images from a local folder\n",
    "# dataset = load_dataset(\"imagefolder\", data_dir=\"path/to/folder\")\n",
    "\n",
    "# We'll train on 32-pixel square images, but you can try larger sizes too\n",
    "image_size = 32\n",
    "# You can lower your batch size if you're running out of GPU memory\n",
    "batch_size = 64\n",
    "\n",
    "# Define data augmentations\n",
    "preprocess = transforms.Compose(\n",
    "    [\n",
    "        transforms.Resize((image_size, image_size)),  # Resize\n",
    "        transforms.RandomHorizontalFlip(),  # Randomly flip (data augmentation)\n",
    "        transforms.ToTensor(),  # Convert to tensor (0, 1)\n",
    "        transforms.Normalize([0.5], [0.5]),  # Map to (-1, 1)\n",
    "    ]\n",
    ")\n",
    "\n",
    "\n",
    "def transform(examples):\n",
    "    images = [preprocess(image.convert(\"RGB\")) for image in examples[\"image\"]]\n",
    "    return {\"images\": images}\n",
    "\n",
    "\n",
    "dataset.set_transform(transform)\n",
    "\n",
    "# Create a dataloader from the dataset to serve up the transformed images in batches\n",
    "train_dataloader = DataLoader(dataset, batch_size=batch_size, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "xb = next(iter(train_dataloader))[\"images\"].to(device)[:8]\n",
    "print(\"X shape:\", xb.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from diffusers import DDPMScheduler\n",
    "\n",
    "noise_scheduler = DDPMScheduler(num_train_timesteps=1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from diffusers import UNet2DModel\n",
    "\n",
    "# Create a model\n",
    "model = UNet2DModel(\n",
    "    sample_size=image_size,  # the target image resolution\n",
    "    in_channels=3,  # the number of input channels, 3 for RGB images\n",
    "    out_channels=3,  # the number of output channels\n",
    "    layers_per_block=2,  # how many ResNet layers to use per UNet block\n",
    "    block_out_channels=(64, 128, 128, 256),  # More channels -> more parameters\n",
    "    down_block_types=(\n",
    "        \"DownBlock2D\",  # a regular ResNet downsampling block\n",
    "        \"DownBlock2D\",\n",
    "        \"AttnDownBlock2D\",  # a ResNet downsampling block with spatial self-attention\n",
    "        \"AttnDownBlock2D\",\n",
    "    ),\n",
    "    up_block_types=(\n",
    "        \"AttnUpBlock2D\",\n",
    "        \"AttnUpBlock2D\",  # a ResNet upsampling block with spatial self-attention\n",
    "        \"UpBlock2D\",\n",
    "        \"UpBlock2D\",  # a regular ResNet upsampling block\n",
    "    ),\n",
    ")\n",
    "model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Set the noise scheduler\n",
    "# noise_scheduler = DDPMScheduler(num_train_timesteps=1000, beta_schedule=\"squaredcos_cap_v2\")\n",
    "\n",
    "# # Training loop\n",
    "# optimizer = torch.optim.AdamW(model.parameters(), lr=4e-4)\n",
    "\n",
    "# losses = []\n",
    "\n",
    "# for epoch in range(30):\n",
    "#     for step, batch in tqdm(enumerate(train_dataloader), total=len(train_dataloader)):\n",
    "#         clean_images = batch[\"images\"].to(device)\n",
    "#         # Sample noise to add to the images\n",
    "#         noise = torch.randn(clean_images.shape).to(clean_images.device)\n",
    "#         bs = clean_images.shape[0]\n",
    "\n",
    "#         # Sample a random timestep for each image\n",
    "#         timesteps = torch.randint(0, noise_scheduler.num_train_timesteps, (bs,), device=clean_images.device).long()\n",
    "\n",
    "#         # Add noise to the clean images according to the noise magnitude at each timestep\n",
    "#         noisy_images = noise_scheduler.add_noise(clean_images, noise, timesteps)\n",
    "\n",
    "#         # Get the model prediction\n",
    "#         noise_pred = model(noisy_images, timesteps, return_dict=False)[0]\n",
    "\n",
    "#         # Calculate the loss\n",
    "#         loss = F.mse_loss(noise_pred, noise)\n",
    "#         loss.backward(loss)\n",
    "#         losses.append(loss.item())\n",
    "\n",
    "#         # Update the model parameters with the optimizer\n",
    "#         optimizer.step()\n",
    "#         optimizer.zero_grad()\n",
    "\n",
    "#     if (epoch + 1) % 5 == 0:\n",
    "#         loss_last_epoch = sum(losses[-len(train_dataloader) :]) / len(train_dataloader)\n",
    "#         print(f\"Epoch:{epoch+1}, loss: {loss_last_epoch}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DiffusionModelPL(pl.LightningModule):\n",
    "    def __init__(self, model, noise_scheduler):\n",
    "        super().__init__()\n",
    "        self.model = model\n",
    "        self.noise_scheduler = noise_scheduler\n",
    "\n",
    "    def training_step(self, batch, batch_idx):\n",
    "        # 获取干净图像并生成噪声\n",
    "        clean_images = batch[\"images\"]\n",
    "        noise = torch.randn_like(clean_images)\n",
    "        batch_size = clean_images.shape[0]\n",
    "\n",
    "        # 生成随机时间步\n",
    "        timesteps = torch.randint(\n",
    "            0, self.noise_scheduler.num_train_timesteps,\n",
    "            (batch_size,),\n",
    "            device=self.device\n",
    "        ).long()\n",
    "\n",
    "        # 添加噪声\n",
    "        noisy_images = self.noise_scheduler.add_noise(clean_images, noise, timesteps)\n",
    "        \n",
    "        # 模型预测\n",
    "        noise_pred = self.model(noisy_images, timesteps, return_dict=False)[0]\n",
    "        \n",
    "        # 计算损失\n",
    "        loss = F.mse_loss(noise_pred, noise)\n",
    "        self.log(\"train_loss\", loss, prog_bar=True)\n",
    "        return loss\n",
    "\n",
    "    def configure_optimizers(self):\n",
    "        return torch.optim.AdamW(self.model.parameters(), lr=4e-4)\n",
    "\n",
    "    def on_train_epoch_end(self):\n",
    "        # 每5个epoch打印一次平均损失\n",
    "        if (self.current_epoch + 1) % 5 == 0:\n",
    "            avg_loss = self.trainer.callback_metrics[\"train_loss\"]\n",
    "            print(f\"Epoch: {self.current_epoch+1}, Loss: {avg_loss:.4f}\")\n",
    "\n",
    "pl_model = DiffusionModelPL(model, noise_scheduler)\n",
    "\n",
    "# 创建训练器并训练\n",
    "trainer = pl.Trainer(\n",
    "    max_epochs=30,\n",
    "    accelerator=\"auto\",  # 自动选择可用加速器（GPU/TPU等）\n",
    "    devices=\"auto\"        # 自动选择可用设备\n",
    ")\n",
    "trainer.fit(pl_model, train_dataloaders=train_dataloader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from diffusers import DDPMPipeline\n",
    "\n",
    "image_pipe = DDPMPipeline(unet=model, scheduler=noise_scheduler).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pipeline_output = image_pipe()\n",
    "generated_image = pipeline_output.images[0]\n",
    "generated_image"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "torch",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
